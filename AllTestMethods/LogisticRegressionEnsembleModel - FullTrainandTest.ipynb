{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_Train Shape:\n",
      "(66173, 105)\n",
      "Label Shape:\n",
      "(66173, 27)\n",
      "       gk_positioning  ls  st  rs  lw  lf  cf  rf  rw  lam  ...  lwb  ldm  \\\n",
      "58423               1   0   0   0   0   0   0   0   0    0  ...    0    0   \n",
      "27281               0   0   0   0   0   0   0   0   0    0  ...    0    1   \n",
      "56728               0   1   1   1   0   0   0   0   0    0  ...    0    0   \n",
      "74616               0   1   1   1   1   1   1   1   1    0  ...    0    0   \n",
      "42736               0   0   0   0   0   0   0   0   0    0  ...    0    0   \n",
      "\n",
      "       cdm  rdm  rwb  lb  lcb  cb  rcb  rb  \n",
      "58423    0    0    0   0    0   0    0   0  \n",
      "27281    1    1    0   0    1   1    1   0  \n",
      "56728    0    0    0   0    0   0    0   0  \n",
      "74616    0    0    0   0    0   0    0   0  \n",
      "42736    0    0    0   1    0   0    0   1  \n",
      "\n",
      "[5 rows x 27 columns]\n",
      "['gk_positioning', 'ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram', 'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb', 'lb', 'lcb', 'cb', 'rcb', 'rb']\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "#Juliana\n",
    "#Logist Regressiion Ensemble Model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "training_labels = pd.read_csv('/Users/julianashihadeh/Desktop/GroupProject/Player_Position_Prediction/FridayVersion/Player_Position_Prediction/train_labels.csv', sep=\",\",header=\"infer\", low_memory = False)         \n",
    "#print(training_labels.head())\n",
    "\n",
    "#Extracting all the training data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "training_data = pd.read_csv('/Users/julianashihadeh/Desktop/GroupProject/Player_Position_Prediction/SundayVersion/Player_Position_Prediction/train1.csv', sep=\",\",header=\"infer\", low_memory=False)\n",
    "#print(training_data.head())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(training_data, training_labels, test_size=0.2, random_state=11)\n",
    "\n",
    "print(\"X_Train Shape:\")\n",
    "print(X_train.shape)\n",
    "\n",
    "print(\"Label Shape:\")\n",
    "print(y_train.shape)\n",
    "\n",
    "print(y_train.head())\n",
    "\n",
    "#print(y_train)                                                    \n",
    "col_names = y_train.columns.values.tolist()\n",
    "print(col_names)\n",
    "print(len(col_names))\n",
    "\n",
    "#For Training:\n",
    "models = []\n",
    "for i in range(0, 27):\n",
    "    modelname = 'LRModel' + str(i)\n",
    "    #Add code above in here\n",
    "    \n",
    "    #Change the last two lines so they save to separate models\n",
    "    logreg = LogisticRegression(solver='lbfgs', max_iter = 100000)\n",
    "    test_labels = y_train[col_names[i]]\n",
    "    modelname = logreg.fit(X_train, test_labels)\n",
    "    \n",
    "    #At the end, append to the list of models\n",
    "    models.append(modelname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy results and info for player position: gk_positioning\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 1.0\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00     14695\n",
      "           1       1.00      1.00      1.00      1849\n",
      "\n",
      "   micro avg       1.00      1.00      1.00     16544\n",
      "   macro avg       1.00      1.00      1.00     16544\n",
      "weighted avg       1.00      1.00      1.00     16544\n",
      "\n",
      "Model accuracy results and info for player position: ls\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.979086073501\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     14201\n",
      "           1       0.93      0.92      0.93      2343\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     16544\n",
      "   macro avg       0.96      0.96      0.96     16544\n",
      "weighted avg       0.98      0.98      0.98     16544\n",
      "\n",
      "Model accuracy results and info for player position: st\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.979086073501\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     14201\n",
      "           1       0.93      0.92      0.93      2343\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     16544\n",
      "   macro avg       0.96      0.96      0.96     16544\n",
      "weighted avg       0.98      0.98      0.98     16544\n",
      "\n",
      "Model accuracy results and info for player position: rs\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.979086073501\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     14201\n",
      "           1       0.93      0.92      0.93      2343\n",
      "\n",
      "   micro avg       0.98      0.98      0.98     16544\n",
      "   macro avg       0.96      0.96      0.96     16544\n",
      "weighted avg       0.98      0.98      0.98     16544\n",
      "\n",
      "Model accuracy results and info for player position: lw\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.933691972921\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96     13434\n",
      "           1       0.83      0.81      0.82      3110\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     16544\n",
      "   macro avg       0.89      0.89      0.89     16544\n",
      "weighted avg       0.93      0.93      0.93     16544\n",
      "\n",
      "Model accuracy results and info for player position: lf\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.938950676983\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97     15425\n",
      "           1       0.61      0.27      0.37      1119\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     16544\n",
      "   macro avg       0.78      0.63      0.67     16544\n",
      "weighted avg       0.93      0.94      0.93     16544\n",
      "\n",
      "Model accuracy results and info for player position: cf\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.938950676983\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97     15425\n",
      "           1       0.61      0.27      0.37      1119\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     16544\n",
      "   macro avg       0.78      0.63      0.67     16544\n",
      "weighted avg       0.93      0.94      0.93     16544\n",
      "\n",
      "Model accuracy results and info for player position: rf\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.938950676983\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.99      0.97     15425\n",
      "           1       0.61      0.27      0.37      1119\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     16544\n",
      "   macro avg       0.78      0.63      0.67     16544\n",
      "weighted avg       0.93      0.94      0.93     16544\n",
      "\n",
      "Model accuracy results and info for player position: rw\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.933691972921\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96     13434\n",
      "           1       0.83      0.81      0.82      3110\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     16544\n",
      "   macro avg       0.89      0.89      0.89     16544\n",
      "weighted avg       0.93      0.93      0.93     16544\n",
      "\n",
      "Model accuracy results and info for player position: lam\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.92915860735\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96     14607\n",
      "           1       0.75      0.60      0.66      1937\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     16544\n",
      "   macro avg       0.85      0.79      0.81     16544\n",
      "weighted avg       0.92      0.93      0.93     16544\n",
      "\n",
      "Model accuracy results and info for player position: cam\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.92915860735\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96     14607\n",
      "           1       0.75      0.60      0.66      1937\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     16544\n",
      "   macro avg       0.85      0.79      0.81     16544\n",
      "weighted avg       0.92      0.93      0.93     16544\n",
      "\n",
      "Model accuracy results and info for player position: ram\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.92915860735\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.97      0.96     14607\n",
      "           1       0.75      0.60      0.66      1937\n",
      "\n",
      "   micro avg       0.93      0.93      0.93     16544\n",
      "   macro avg       0.85      0.79      0.81     16544\n",
      "weighted avg       0.92      0.93      0.93     16544\n",
      "\n",
      "Model accuracy results and info for player position: lm\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.89621615087\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94     13882\n",
      "           1       0.71      0.60      0.65      2662\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     16544\n",
      "   macro avg       0.82      0.78      0.79     16544\n",
      "weighted avg       0.89      0.90      0.89     16544\n",
      "\n",
      "Model accuracy results and info for player position: lcm\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.947231624758\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97     15461\n",
      "           1       0.64      0.45      0.53      1083\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     16544\n",
      "   macro avg       0.80      0.72      0.75     16544\n",
      "weighted avg       0.94      0.95      0.94     16544\n",
      "\n",
      "Model accuracy results and info for player position: cm\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.947171179884\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97     15461\n",
      "           1       0.63      0.45      0.53      1083\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     16544\n",
      "   macro avg       0.80      0.72      0.75     16544\n",
      "weighted avg       0.94      0.95      0.94     16544\n",
      "\n",
      "Model accuracy results and info for player position: rcm\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.947231624758\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97     15461\n",
      "           1       0.64      0.45      0.53      1083\n",
      "\n",
      "   micro avg       0.95      0.95      0.95     16544\n",
      "   macro avg       0.80      0.72      0.75     16544\n",
      "weighted avg       0.94      0.95      0.94     16544\n",
      "\n",
      "Model accuracy results and info for player position: rm\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.896155705996\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94     13882\n",
      "           1       0.71      0.60      0.65      2662\n",
      "\n",
      "   micro avg       0.90      0.90      0.90     16544\n",
      "   macro avg       0.82      0.78      0.79     16544\n",
      "weighted avg       0.89      0.90      0.89     16544\n",
      "\n",
      "Model accuracy results and info for player position: lwb\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.936835106383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96     14450\n",
      "           1       0.76      0.73      0.74      2094\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     16544\n",
      "   macro avg       0.86      0.85      0.85     16544\n",
      "weighted avg       0.94      0.94      0.94     16544\n",
      "\n",
      "Model accuracy results and info for player position: ldm\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.944088491296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97     15061\n",
      "           1       0.72      0.62      0.66      1483\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     16544\n",
      "   macro avg       0.84      0.80      0.82     16544\n",
      "weighted avg       0.94      0.94      0.94     16544\n",
      "\n",
      "Model accuracy results and info for player position: cdm\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.944028046422\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97     15061\n",
      "           1       0.72      0.62      0.66      1483\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     16544\n",
      "   macro avg       0.84      0.80      0.82     16544\n",
      "weighted avg       0.94      0.94      0.94     16544\n",
      "\n",
      "Model accuracy results and info for player position: rdm\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.944088491296\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97     15061\n",
      "           1       0.72      0.62      0.66      1483\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     16544\n",
      "   macro avg       0.84      0.80      0.82     16544\n",
      "weighted avg       0.94      0.94      0.94     16544\n",
      "\n",
      "Model accuracy results and info for player position: rwb\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.936835106383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.96     14450\n",
      "           1       0.76      0.73      0.74      2094\n",
      "\n",
      "   micro avg       0.94      0.94      0.94     16544\n",
      "   macro avg       0.86      0.85      0.85     16544\n",
      "weighted avg       0.94      0.94      0.94     16544\n",
      "\n",
      "Model accuracy results and info for player position: lb\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.923476789168\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96     14728\n",
      "           1       0.67      0.59      0.63      1816\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     16544\n",
      "   macro avg       0.81      0.78      0.79     16544\n",
      "weighted avg       0.92      0.92      0.92     16544\n",
      "\n",
      "Model accuracy results and info for player position: lcb\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.970200676983\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     12306\n",
      "           1       0.95      0.94      0.94      4238\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     16544\n",
      "   macro avg       0.96      0.96      0.96     16544\n",
      "weighted avg       0.97      0.97      0.97     16544\n",
      "\n",
      "Model accuracy results and info for player position: cb\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.970200676983\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     12306\n",
      "           1       0.95      0.94      0.94      4238\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     16544\n",
      "   macro avg       0.96      0.96      0.96     16544\n",
      "weighted avg       0.97      0.97      0.97     16544\n",
      "\n",
      "Model accuracy results and info for player position: rcb\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.970261121857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98     12306\n",
      "           1       0.95      0.94      0.94      4238\n",
      "\n",
      "   micro avg       0.97      0.97      0.97     16544\n",
      "   macro avg       0.96      0.96      0.96     16544\n",
      "weighted avg       0.97      0.97      0.97     16544\n",
      "\n",
      "Model accuracy results and info for player position: rb\n",
      "The number of values in a test_label:\n",
      "16544\n",
      "Accuracy: 0.923476789168\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.96      0.96     14728\n",
      "           1       0.67      0.59      0.63      1816\n",
      "\n",
      "   micro avg       0.92      0.92      0.92     16544\n",
      "   macro avg       0.81      0.78      0.79     16544\n",
      "weighted avg       0.92      0.92      0.92     16544\n",
      "\n",
      "Length of all_f1scores and all_accuracies:\n",
      "27\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "#Testing\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "#For Testing: \n",
    "all_predictions = []\n",
    "all_f1scores = []\n",
    "all_accuracies = []\n",
    "for i in range(0, 27):\n",
    "    #Select the model\n",
    "    #modelname = 'model' + str(i)\n",
    "    #Test the model. Pass all the test data, not one by one returns labels of all the test data\n",
    "    print(\"Model accuracy results and info for player position: \" + col_names[i])\n",
    "    predictions = models[i].predict(X_test)\n",
    "    test_labels = y_test[col_names[i]]\n",
    "    print(\"The number of values in a test_label:\")\n",
    "    print(len(test_labels))\n",
    "    a = metrics.accuracy_score(test_labels, predictions)\n",
    "    print(\"Accuracy:\",a)\n",
    "    print(classification_report(test_labels, predictions))\n",
    "    f1 = f1_score(test_labels, predictions, average='weighted')\n",
    "    all_f1scores.append(f1)\n",
    "    all_accuracies.append(a)\n",
    "    #Final shape should be 32 X 82717: 32 lists of 82717 length \n",
    "    all_predictions.append(predictions)\n",
    "\n",
    "\n",
    "print(\"Length of all_f1scores and all_accuracies:\")\n",
    "print(len(all_f1scores))\n",
    "print(len(all_accuracies))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(all_predictions, index = ['gk_positioning', 'ls', 'st', 'rs', 'lw', 'lf', 'cf', 'rf', 'rw', 'lam', 'cam', 'ram', 'lm', 'lcm', 'cm', 'rcm', 'rm', 'lwb', 'ldm', 'cdm', 'rdm', 'rwb', 'lb', 'lcb', 'cb', 'rcb', 'rb'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Traverse label pandas dataframe to prepare for the postprocessing\n",
    "addT = df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.941849722774\n"
     ]
    }
   ],
   "source": [
    "#Averaging the weighted f1 scores to compute overall f1 score on the entire Logistic Regression Ensemble\n",
    "len(all_f1scores)\n",
    "s = 0.0\n",
    "for i in all_f1scores:\n",
    "    s = s + i\n",
    "final_acc = s / len(all_f1scores)\n",
    "print(final_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Juliana \n",
    "#All Post Processing Steps\n",
    "#Convert classifications of 1 and 0s to the corresponding classes\n",
    "#Match all players that can substitue a position\n",
    "#Recommend the top 3 players based on player's overall score\n",
    "#To Do: Finish the positions dictionary\n",
    "\n",
    "#Pass in the player position and the predictions list\n",
    "def matchingplayers(player_position, predictions, test_data, modelname):\n",
    "    \n",
    "    #Create a dictionary of all the player_positions and what they stand for\n",
    "    positions = {\"cb\": 'Centre back',\n",
    "                \"cdm\": 'Central defensive midfielder',\n",
    "                \"cf\": 'Center forward',\n",
    "                \"cm\": 'Centre midfielder',\n",
    "                \"gk\": 'Goalkeeper',\n",
    "                \"lb\": 'Left back',\n",
    "                \"lm\": 'Left midfielder',\n",
    "                \"lw\": 'Left winger',\n",
    "                \"rb\": 'Right back',\n",
    "                \"rm\": 'Right midfielder',\n",
    "                \"rw\": 'Right winger',\n",
    "                \"st\": 'Striker',\n",
    "                \"cb\": 'Center-back',\n",
    "                \"lcb\": 'Left center-back',\n",
    "                \"rcb\": 'Right center-back',\n",
    "                \"lb\": 'Left-back',\n",
    "                \"rb\": 'Right-back',\n",
    "                \"cm\": 'Centre midfield',\n",
    "                \"ldm\": 'Left defense midfielder',\n",
    "                \"lam\": 'Left attacking midfield',\n",
    "                \"rdm\": 'Right defense midfield',\n",
    "                \"ram\": 'Right attacking midfield',\n",
    "                \"cdm\": 'Centre defensive midfield',\n",
    "                \"cam\": 'Centre attacking midfield',\n",
    "                \"lm\" : 'Left midfield',\n",
    "                \"rm\" : 'Right midfield',\n",
    "                \"st\": 'Striker',\n",
    "                \"cf\": 'Center forward',\n",
    "                \"lw\": 'Left winger',\n",
    "                \"rw\": 'Right winger', \n",
    "                \"ls\": 'Left striking',\n",
    "                \"rs\": 'Right striking', \n",
    "                \"lf\": 'Left forward', \n",
    "                \"rf\": 'Right forward',\n",
    "                \"lcm\": 'Left center midfielder',\n",
    "                \"rcm\": 'Right center midfielder',\n",
    "                \"lwb\": 'Left wing back',\n",
    "                \"rwb\": 'Rigth wing back'}\n",
    "    #Lists\n",
    "    cm = [\"cm\", \"ldm\", \"lam\", \"rdm\", \"ram\", \"cdm\", \"cam\", \"lm\" , \"rm\"]\n",
    "    attack = [\"sf\", \"cf\", \"lw\", \"rw\" , \"ls\", \"rs\"]\n",
    "    defense = [\"defense\", \"cb\", \"lcb\", \"rcb\", \"lb\", \"rb\"]   \n",
    "    lb = [\"cb\", \"cdm\",\"cf\", \"cm\", \"lb\", \"lm\", \"lw\", \"rb\", \"rm\", \"rw\", \"st\"]\n",
    "    gk = ['gk']\n",
    "    matches = []\n",
    "    position_name = positions.get(player_position) \n",
    "    file_name = (modelname + \"_Positions_A_Player_Can_Substitute_For_\" + position_name + \".txt\")\n",
    "    outputFile = open(file_name,'w')\n",
    "    for i in range(len(predictions)):\n",
    "        for j in range(len(predictions[i])):\n",
    "            if predictions[i][j] == player_position:\n",
    "                #Look up the player's name from the original pandas data frame\n",
    "                player_name = test_data.loc[i, 'long_name']\n",
    "                #print(\"The player's name:\")\n",
    "                #print(player_name)\n",
    "                #position_name = positions.get(player_position) \n",
    "                #print(\"The position name:\")\n",
    "                #print(position_name)\n",
    "                if player_position in cm:\n",
    "                    type_position = \"defense\"\n",
    "                elif player_position in attack:\n",
    "                    type_position = \"attacker\"\n",
    "                elif player_position in defense:\n",
    "                    type_position = \"defense\"\n",
    "                elif player_position in lb:\n",
    "                    type_position = \"defense\"\n",
    "                elif player_position in gk:\n",
    "                    type_position = 'goal keeper'\n",
    "                print(player_name + \" can substitute for the position \" + player_position + \", \" + position_name + \" position, which is a(n) \" + type_position + \" position. \\n\")\n",
    "                outputFile.writelines(player_name + \" can substitute for the position \" + player_position + \", \" + position_name + \" position, which is a(n) \" + type_position + \" position. \\n\")\n",
    "                #player = player_name[i] \n",
    "                matches.append(player_name)\n",
    "    topthreeplayers(len(predictions), test_data, matches, position_name, modelname)\n",
    "\n",
    "#Pass the predictions list and the dataframe of the test data\n",
    "def topthreeplayers(total_predictions, df, matches, position, modelname):  \n",
    "        overall_scores_dictionary = {}\n",
    "        for i in matches:\n",
    "            #print(\"In MATCHES LOOP\")\n",
    "            #print(i)\n",
    "            r = df.loc[df['long_name'] == i]\n",
    "            #print(r)\n",
    "            ind = r.index\n",
    "            t = df.get_value( ind , 3 , takeable = True) \n",
    "            overall_score = t[0]\n",
    "            #print(\"The overall score of the player\")\n",
    "            #print(overall_score)\n",
    "\n",
    "    \n",
    "            #print(\"The index is:\")\n",
    "            #print(i)\n",
    "            overall_scores_dictionary[i] = overall_score\n",
    "            \n",
    "        #Sort dictionary by value\n",
    "        #from collections import OrderedDict\n",
    "        #sorted_scores = OrderedDict(sorted(overall_scores_dictionary.items(), key=lambda x: x[1]))\n",
    "        sorted_scores = sorted(overall_scores_dictionary.items() ,  key=lambda x: x[1])\n",
    "        \n",
    "        print('\\n')\n",
    "        file_name = (modelname + \"_Roster.txt\")\n",
    "        outputFile = open(file_name,'w')\n",
    "        print(\"The entire roster of players who can be swapped for the \" + '\\033[1m' + position + '\\033[0m' + \" position in order of best overall performance to least based on the \" + modelname + \"are:\")\n",
    "        roster = []\n",
    "        outputFile.writelines(\"The entire roster of players who can be swapped for the \"+ position + \" position in order of best overall performance to least based on the \" + modelname + \"are:\")\n",
    "        for i in sorted_scores:\n",
    "            roster.append(i[0])\n",
    "            outputFile.writelines(i[0])\n",
    "            outputFile.writelines('\\n')\n",
    "            print(i[0])\n",
    "        outputFile.close()\n",
    "        \n",
    "        \n",
    "        l = len(sorted_scores)\n",
    "        #ids of top 3\n",
    "        print(\"The 3 Players with the highest overall performance score from all players who can be a subtitue for the \" + '\\033[1m' + position + '\\033[0m' + \" position are: \" )\n",
    "        id_top_1 = sorted_scores[l-1][0]\n",
    "        print(\"1. \" + '\\033[1m' + id_top_1 + '\\033[0m' + \" with an overall score: \" + '\\033[1m' + str(sorted_scores[l-1][1]) + '\\033[0m')\n",
    "        id_top_2 = sorted_scores[l-2][0]\n",
    "        print(\"2. \" + '\\033[1m' + id_top_2 + '\\033[0m' + \" with an overall score: \" + '\\033[1m' + str(sorted_scores[l-2][1]) + '\\033[0m')\n",
    "        id_top_3 = sorted_scores[l-3][0]\n",
    "        print(\"3. \" + '\\033[1m' + id_top_3 + '\\033[0m' + \" with an overall score: \" + '\\033[1m' + str(sorted_scores[l-3][1]) + '\\033[0m')\n",
    "        \n",
    "        #print(roster)\n",
    "           \n",
    "        #Looks up the specifics names of the players\n",
    "        #name_top_1 = df.loc[id_top_1, 'long_name']\n",
    "        #name_top_2 = df.loc[id_top_2, 'long_name']\n",
    "        #name_top_3 = df.loc[id_top_3, 'long_name']\n",
    "        \n",
    "        #Creates a data frame of the final resultls\n",
    "        #row_labels = [id_top_1, id_top_2, id_top_3]\n",
    "        #col_labels = ['long_name']\n",
    "        #final_results = df.lookup(self, row_labels, col_labels)\n",
    "        #print(\"The top 3 recommended players are: \\n1. \" + id_top_1 + \"\\n2. \" + id_top_2 + \"\\n3. \" + id_top_3 + \"\\n\")\n",
    "\n",
    "#Pass the final data frame that holds all the class informatiion\n",
    "def apply_postprocessing(pred_df, modelname, position):\n",
    "    #Following line of code based off of Apeksha's Code From Feature Extraction\n",
    "    labels=pred_df[pred_df.apply(lambda x: x == pred_df.max(axis = 1))] \n",
    "    #print(labels)\n",
    "    classes=[]\n",
    "\n",
    "    #Write all labels to file named model\n",
    "    file_name = modelname + '.txt'\n",
    "    outputFile = open(file_name,'w')\n",
    "\n",
    "    for i in range(labels.shape[0]):\n",
    "        #Following line of code based off of Apeksha's Code From Feature Extraction\n",
    "        l=list(labels.columns[np.isfinite(labels.iloc[i])])\n",
    "        #print(l)\n",
    "        formatted_l=map(lambda x:x+' ', l)\n",
    "        #Eventually add in the players's name too\n",
    "        outputFile.writelines(\"ID: \" + str(i) + \", Total Classes: \" + str(len(l)) + \" Classes: \")\n",
    "        outputFile.writelines(formatted_l)\n",
    "        outputFile.writelines('\\n')\n",
    "        #for i in l:\n",
    "        #    outputFile.write(i)\n",
    "        #    outputFile.write('\\n')\n",
    "        classes.append(l)\n",
    "        #Save the classes to a file\n",
    "    print(\"Number of classes:\")\n",
    "    print(len(classes))\n",
    "    #print(classes)\n",
    "    outputFile.close()\n",
    "    \n",
    "    #Import the dataframe of test data\n",
    "    #Getting the test data\n",
    "    import pandas as pd\n",
    "    test_data = pd.read_csv('/Users/julianashihadeh/Desktop/GroupProject/Player_Position_Prediction/MondayVersion/Player_Position_Prediction/names.csv', sep=\",\",header=\"infer\")\n",
    "    print(\"TEST DATA READ\")\n",
    "    #print(test_data.head())\n",
    "\n",
    "    predictions = classes    \n",
    "    number_of_predictions = len(classes)\n",
    "    #Pass a player position\n",
    "    #Match all players that can substitute a position\n",
    "    matchingplayers(position, predictions, test_data, modelname)\n",
    "    #List top 3 players that can substitue a position\n",
    "    #topthreeplayers(number_of_predictions, test_data)\n",
    "    \n",
    "#Pass the final Data Frame of labels, the model name, and the position \n",
    "apply_postprocessing(addT, \"LogistRegressionEnsemble\", 'gk')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
